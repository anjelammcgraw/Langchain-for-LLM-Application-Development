{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjelammcgraw/Langchain-for-LLM-Application-Development/blob/main/CL1_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain: Models, Prompts and Output Parsers\n",
        "### Outline:\n",
        "* Direct API calls to OpenAI\n",
        "* API calls through LangChain:\n",
        "* Prompts\n",
        "* Models\n",
        "* Output parsers"
      ],
      "metadata": {
        "id": "01i-x_nIMXpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get your Open AI API Key"
      ],
      "metadata": {
        "id": "EvgxStaFMoak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai langchain langchain_core langchain-community langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y7On5DoOzDH",
        "outputId": "57cb1b53-ebb0-4a10-f018-1fe05d1b3845"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsZ1oOZnMv4V",
        "outputId": "38c94247-59ec-46dc-dc0a-bd8d7e4cfca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# account for deprecation of LLM model\n",
        "import datetime\n",
        "# Get the current date\n",
        "current_date = datetime.datetime.now().date()\n",
        "\n",
        "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
        "target_date = datetime.date(2024, 6, 12)\n",
        "\n",
        "# Set the model variable based on the current date\n",
        "if current_date > target_date:\n",
        "    llm_model = \"gpt-3.5-turbo\"\n",
        "else:\n",
        "    llm_model = \"gpt-3.5-turbo-0301\""
      ],
      "metadata": {
        "id": "YgPkycj9MyK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat API : OpenAI\n",
        "Let's start with a direct API calls to OpenAI."
      ],
      "metadata": {
        "id": "VY7cnQ_lM4wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = openai.OpenAI()\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "4BiwmNXiM19p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1cc57b2"
      },
      "outputs": [],
      "source": [
        "response = get_completion(\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76774108",
        "outputId": "0c9918d4-ab7f-425f-9147-81a0ba668f4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1+1 equals 2.\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse,\\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"Translate the text \\\n",
        "that is delimited by triple backticks\n",
        "into a style that is {style}.\n",
        "text: ```{customer_email}```\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "response = get_completion(prompt)\n",
        "\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "RI5udqUtNIJk",
        "outputId": "0956f60d-082d-48b7-f061-5224af20a72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks\n",
            "into a style that is American English in a calm and respectful tone\n",
            ".\n",
            "text: ```\n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "```\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Oh man, I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And on top of that, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, buddy!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJamnsucPIIP",
        "outputId": "4789a474-0d06-47d3-9b90-be64b24ee53f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks\n",
            "into a style that is American English in a calm and respectful tone\n",
            ".\n",
            "text: ```\n",
            "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = get_completion(prompt)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHuJWARUPKEC",
        "outputId": "d628786d-5a96-4e3f-e245-90d81105c602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chat API : LangChain\n",
        "Let's try how we can do the same using LangChain."
      ],
      "metadata": {
        "id": "bAWNwzvqNRre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFvzkbvEPkj7",
        "outputId": "c7920622-6f45-4a5e-adba-a6f360d7cdb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.5)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.34)\n",
            "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.46)\n",
            "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.52)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model\n"
      ],
      "metadata": {
        "id": "Omrd1ztkPlQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# To control the randomness and creativity of the generated\n",
        "# text by an LLM, use temperature = 0.0\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39JkuYKONgoc",
        "outputId": "38709fd2-da1d-41a2-9403-5a8815270e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7ce12e62b520>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7ce12e470c70>, model_name='gpt-3.5-turbo-0301', temperature=0.0, openai_api_key='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', openai_proxy='')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prompt template"
      ],
      "metadata": {
        "id": "NliN6kVwNhMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Bej_pgLQNqgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "\n",
        "prompt_template.messages[0].prompt\n",
        "\n",
        "prompt_template.messages[0].prompt.input_variables\n",
        "\n",
        "customer_style = \"\"\"American English \\\n",
        "in a calm and respectful tone\n",
        "\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Arrr, I be fuming that me blender lid \\\n",
        "flew off and splattered me kitchen walls \\\n",
        "with smoothie! And to make matters worse, \\\n",
        "the warranty don't cover the cost of \\\n",
        "cleaning up me kitchen. I need yer help \\\n",
        "right now, matey!\n",
        "\"\"\"\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "print(type(customer_messages))\n",
        "print(type(customer_messages[0]))\n",
        "\n",
        "print(customer_messages[0])\n",
        "\n",
        "# Call the LLM to translate to the style of the customer message\n",
        "customer_response = chat(customer_messages)\n",
        "\n",
        "print(customer_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ia0V9ojzNvYz",
        "outputId": "f10fbd30-a9b7-40fd-f98b-458bbb5936eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'langchain_core.messages.human.HumanMessage'>\n",
            "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone\\n. text: ```\\nArrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\\n\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! To add insult to injury, the warranty doesn't cover the cost of cleaning up my kitchen. Can you please help me out, friend?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "service_reply = \"\"\"Hey there customer, \\\n",
        "the warranty does not cover \\\n",
        "cleaning expenses for your kitchen \\\n",
        "because it's your fault that \\\n",
        "you misused your blender \\\n",
        "by forgetting to put the lid on before \\\n",
        "starting the blender. \\\n",
        "Tough luck! See ya!\n",
        "\"\"\"\n",
        "\n",
        "service_style_pirate = \"\"\"\\\n",
        "a polite tone \\\n",
        "that speaks in English Pirate\\\n",
        "\"\"\"\n",
        "\n",
        "service_messages = prompt_template.format_messages(\n",
        "    style=service_style_pirate,\n",
        "    text=service_reply)\n",
        "\n",
        "print(service_messages[0].content)\n",
        "\n",
        "service_response = chat(service_messages)\n",
        "print(service_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJt0OmrmNztQ",
        "outputId": "f33e1b3b-9557-46d5-e2c4-1f201cd35d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
            "```\n",
            "\n",
            "Ahoy there, matey! I must kindly inform ye that the warranty be not coverin' the expenses o' cleaning yer galley, as 'tis yer own fault fer misusin' yer blender by forgettin' to put the lid on afore startin' it. Aye, tough luck! Farewell and may the winds be in yer favor!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Output Parsers\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "\n",
        "Let's start with defining how we would like the LLM output to look like:\n",
        "\n",
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9-qOC9BLN4L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"gift\": False,\n",
        "  \"delivery_days\": 5,\n",
        "  \"price_value\": \"pretty affordable!\"\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG9km6LGOEHT",
        "outputId": "7e0699c6-bf08-478d-d4aa-4f30e2109828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_review = \"\"\"\\\n",
        "This leaf blower is pretty amazing.  It has four settings:\\\n",
        "candle blower, gentle breeze, windy city, and tornado. \\\n",
        "It arrived in two days, just in time for my wife's \\\n",
        "anniversary present. \\\n",
        "I think my wife liked it so much she was speechless. \\\n",
        "So far I've been the only one using it, and I've been \\\n",
        "using it every other morning to clear the leaves on our lawn. \\\n",
        "It's slightly more expensive than the other leaf blowers \\\n",
        "out there, but I think it's worth it for the extra features.\n",
        "\"\"\"\n",
        "\n",
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "​\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "​\n",
        "delivery_days: How many days did it take for the product \\\n",
        "to arrive? If this information is not found, output -1.\n",
        "​\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "​\n",
        "Format the output as JSON with the following keys:\n",
        "gift\n",
        "delivery_days\n",
        "price_value\n",
        "​\n",
        "text: {text}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7pxcNyYrOG9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)\n",
        "\n",
        "messages = prompt_template.format_messages(text=customer_review)\n",
        "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "response = chat(messages)\n",
        "print(response.content)\n",
        "\n",
        "type(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpD7ejYkOPVd",
        "outputId": "c7453298-58bd-4e20-a3f0-5d56525e6883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\u200b\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\u200b\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\u200b\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\u200b\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\u200b\\ntext: {text}\\n'))]\n",
            "{\n",
            "    \"gift\": true,\n",
            "    \"delivery_days\": 2,\n",
            "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parse the LLM output string into a Python dictionary"
      ],
      "metadata": {
        "id": "SQKTTXaIOdSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "gift_schema = ResponseSchema(name=\"gift\",\n",
        "                             description=\"Was the item purchased\\\n",
        "                             as a gift for someone else? \\\n",
        "                             Answer True if yes,\\\n",
        "                             False if not or unknown.\")\n",
        "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
        "                                      description=\"How many days\\\n",
        "                                      did it take for the product\\\n",
        "                                      to arrive? If this \\\n",
        "                                      information is not found,\\\n",
        "                                      output -1.\")\n",
        "price_value_schema = ResponseSchema(name=\"price_value\",\n",
        "                                    description=\"Extract any\\\n",
        "                                    sentences about the value or \\\n",
        "                                    price, and output them as a \\\n",
        "                                    comma separated Python list.\")\n",
        "\n",
        "response_schemas = [gift_schema,\n",
        "                    delivery_days_schema,\n",
        "                    price_value_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8gVtaa4OkRS",
        "outputId": "d0d7c213-4a7f-459d-fa37-068687058712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "​\n",
        "gift: Was the item purchased as a gift for someone else? \\\n",
        "Answer True if yes, False if not or unknown.\n",
        "​\n",
        "delivery_days: How many days did it take for the product\\\n",
        "to arrive? If this information is not found, output -1.\n",
        "​\n",
        "price_value: Extract any sentences about the value or price,\\\n",
        "and output them as a comma separated Python list.\n",
        "​\n",
        "text: {text}\n",
        "​\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=customer_review,\n",
        "                                format_instructions=format_instructions)\n",
        "\n",
        "print(messages[0].content)\n",
        "\n",
        "response = chat(messages)\n",
        "\n",
        "print(response.content)\n",
        "\n",
        "output_dict = output_parser.parse(response.content)\n",
        "\n",
        "output_dict\n",
        "\n",
        "type(output_dict)\n",
        "\n",
        "output_dict.get('delivery_days')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrN3iMfROel4",
        "outputId": "1f5f7b87-1b75-4705-8afc-239a2d500f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "​\n",
            "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
            "​\n",
            "delivery_days: How many days did it take for the productto arrive? If this information is not found, output -1.\n",
            "​\n",
            "price_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\n",
            "​\n",
            "text: This leaf blower is pretty amazing.  It has four settings:candle blower, gentle breeze, windy city, and tornado. It arrived in two days, just in time for my wife's anniversary present. I think my wife liked it so much she was speechless. So far I've been the only one using it, and I've been using it every other morning to clear the leaves on our lawn. It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\n",
            "\n",
            "​\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
            "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
            "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
            "}\n",
            "```\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"gift\": true,\n",
            "\t\"delivery_days\": 2,\n",
            "\t\"price_value\": [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]\n",
            "}\n",
            "```\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "L2\n"
      ],
      "metadata": {
        "id": "pk1JBcK2sNgN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a786c77c"
      },
      "source": [
        "# LangChain: Memory\n",
        "\n",
        "## Outline\n",
        "* ConversationBufferMemory\n",
        "* ConversationBufferWindowMemory\n",
        "* ConversationTokenBufferMemory\n",
        "* ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1297dcd5"
      },
      "source": [
        "## ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "a1f518f5"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2301270b-7a69-40f8-9934-f840499b6ae9"
      },
      "source": [
        "Note: LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "20ad6fe2"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "88bdf13d"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferMemory()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "db24677d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "cc830cc4-3751-444a-f8db-b50feab86a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi, my name is Andrew\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "cc3ef937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "852619e0-a6ce-4074-c747-fc7ccfa60858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 1+1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "acf3339a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "8ddfb169-d4ab-4338-d1b3-6038133051c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: The answer to 1+1 is 2.\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Andrew, as you mentioned earlier.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "2529400d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af626d7e-f593-4c0c-c050-820b1bb6fc59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi, my name is Andrew\n",
            "AI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\n",
            "Human: What is 1+1?\n",
            "AI: The answer to 1+1 is 2.\n",
            "Human: What is my name?\n",
            "AI: Your name is Andrew, as you mentioned earlier.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "5018cb0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1af176b-dd22-412f-ce48-db2f26ca214e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\\nHuman: What is my name?\\nAI: Your name is Andrew, as you mentioned earlier.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "14219b70"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "a36e9905"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "61631b1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ca2ece-fc8a-4c45-c5a7-e78ed4127490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi\n",
            "AI: What's up\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "a2fdf9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cced167-7b0c-41b3-e3d3-89761b21b650"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\"}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "7ca79256"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "890a4497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27b0df3-ec44-406d-c7e6-65e15d156fc1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cf98e9ff"
      },
      "source": [
        "## ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "66eeccc3"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "3ea6233e"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferWindowMemory(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "dc4553fb"
      },
      "outputs": [],
      "source": [
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "6a788403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3823fa9c-5f5d-47b1-c1f1-9ca1065caa7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "4087bc87"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "4faaa952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7c28b4e8-0111-4a0e-de38-bb723c8b58c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Andrew, it's nice to meet you. My name is AI. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "conversation.predict(input=\"Hi, my name is Andrew\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "bb20ddaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "610eec7d-9e65-4157-a97e-386d8687bb2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The answer to 1+1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "conversation.predict(input=\"What is 1+1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "489b2194",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dc1d0e09-5152-4936-8e45-d42fe2527fab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2931b92"
      },
      "source": [
        "## ConversationTokenBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "9f6d063c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dacb765f-3f21-4148-fa3e-b4890510487c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "fb9020ed"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.llms import OpenAI\n",
        "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 132,
        "tags": [],
        "id": "43582ee6"
      },
      "outputs": [],
      "source": [
        "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
        "memory.save_context({\"input\": \"AI is what?!\"},\n",
        "                    {\"output\": \"Amazing!\"})\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "284288e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a40e2ea4-2bb8-42e4-d5bd-d7271a850e6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff55d5d"
      },
      "source": [
        "## ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "72dcf8b1"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 268,
        "tags": [],
        "id": "4a5b238f"
      },
      "outputs": [],
      "source": [
        "# create a long string\n",
        "schedule = \"There is a meeting at 8am with your product team. \\\n",
        "You will need your powerpoint presentation prepared. \\\n",
        "9am-12pm have time to work on your LangChain \\\n",
        "project which will go quickly because Langchain is such a powerful tool. \\\n",
        "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
        "from over an hour away to meet you to understand the latest in AI. \\\n",
        "Be sure to bring your laptop to show the latest LLM demo.\"\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"},\n",
        "                    {\"output\": f\"{schedule}\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "2e4ecabe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be6d0c38-91a4-4bcb-8dcb-89a69044b89c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 98,
        "tags": [],
        "id": "6728edba"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "9a221b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "a936f823-ef67-4398-9a6d-761031204cd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\n",
            "Human: What would be a good demo to show?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can accurately understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our AI's ability to learn and adapt over time, making it a valuable tool for businesses looking to improve their customer experience.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "conversation.predict(input=\"What would be a good demo to show?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "bb582617",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488ea072-467f-4378-910b-8b47f2823c34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments. The human asks what would be a good demo to show.\\nAI: Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can accurately understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our AI's ability to learn and adapt over time, making it a valuable tool for businesses looking to improve their customer experience.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "L3"
      ],
      "metadata": {
        "id": "Mob8ZCn0tGta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chains in LangChain\n",
        "* Outline\n",
        "* LLMChain\n",
        "* Sequential Chains\n",
        "* SimpleSequentialChain\n",
        "* SequentialChain\n",
        "* Router Chain\n"
      ],
      "metadata": {
        "id": "JOWMmZf4uVZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4yafyUIufgs",
        "outputId": "9fe735d3-5220-4864-9df1-1508860d77bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IoaQS6b0uiwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/anjelammcgraw/Langchain-for-LLM-Application-Development/main/Data.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8K4WsTTQ3WHv",
        "outputId": "5caa0f82-304c-4a49-f387-a2af71d32ca6"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Product                                             Review\n",
              "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1915601c-79f0-4bde-baab-55b879bebffe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1915601c-79f0-4bde-baab-55b879bebffe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1915601c-79f0-4bde-baab-55b879bebffe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1915601c-79f0-4bde-baab-55b879bebffe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c8791ede-1c77-4789-ae20-c27f428dcc89\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8791ede-1c77-4789-ae20-c27f428dcc89')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c8791ede-1c77-4789-ae20-c27f428dcc89 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Queen Size Sheet Set\",\n          \"Waterproof Phone Pouch\",\n          \"L'Or Espresso Caf\\u00e9\\u00a0\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"I ordered a king size set. My only criticism would be that I wish seller would offer the king size set with 4 pillowcases. I separately ordered a two pack of pillowcases so I could have a total of four. When I saw the two packages, it looked like the color did not exactly match. Customer service was excellent about sending me two more pillowcases so I would have four that matched. Excellent! For the cost of these sheets, I am satisfied with the characteristics and coolness of the sheets.\",\n          \"I loved the waterproof sac, although the opening was made of a hard plastic. I don\\u2019t know if that would break easily. But I couldn\\u2019t turn my phone on, once it was in the pouch.\",\n          \"Je trouve le go\\u00fbt m\\u00e9diocre. La mousse ne tient pas, c'est bizarre. J'ach\\u00e8te les m\\u00eames dans le commerce et le go\\u00fbt est bien meilleur...\\nVieux lot ou contrefa\\u00e7on !?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM Chain"
      ],
      "metadata": {
        "id": "3z61BKnEumx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "product = \"Queen Size Sheet Set\"\n",
        "chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iPnCMiPWutIH",
        "outputId": "f6ad4162-1022-45f1-c678-6a5cbcbb142d"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Royal Bedding Inc.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SimpleSequentialChain"
      ],
      "metadata": {
        "id": "7BY7qL9yuuQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain"
      ],
      "metadata": {
        "id": "mgb_dQTDyoW7"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What is the best name to describe \\\n",
        "    a company that makes {product}?\"\n",
        ")\n",
        "\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
      ],
      "metadata": {
        "id": "69KgAcM1xffU"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a 20 words description for the following \\\n",
        "    company:{company_name}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "tsWhTxx7xg8a"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )"
      ],
      "metadata": {
        "id": "aPVrTO36xumI"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain.run(product)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "xii6fjwtxl_7",
        "outputId": "08b80295-46e8-4f80-abe1-52845531713f"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mHere are a few suggestions:\n",
            "\n",
            "1. Royal Linens\n",
            "2. Majestic Bedding\n",
            "3. Imperial Sheets\n",
            "4. Regal Bedding Co.\n",
            "5. Crowned Comforts\n",
            "6. Sovereign Sheets\n",
            "7. Noble Bedding\n",
            "8. Monarch Linens\n",
            "9. Elite Bedding Co.\n",
            "10. Kingly Sheets\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mA luxury bedding company fit for a king or queen, offering regal sheets and linens for the ultimate in comfort and style.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A luxury bedding company fit for a king or queen, offering regal sheets and linens for the ultimate in comfort and style.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SequentialChain"
      ],
      "metadata": {
        "id": "xg7njm2Zx4NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
        "\n",
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to english:\"\n",
        "    \"\\n\\n{Review}\"\n",
        ")\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"English_Review\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "UJiGrj47w1ol"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\"\n",
        "    \"\\n\\n{English_Review}\"\n",
        ")\n",
        "# chain 2: input= English_Review and output= summary\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"summary\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "7jXBdH8sw6nr"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 3: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"language\"\n",
        "                      )"
      ],
      "metadata": {
        "id": "y_D2xaijw-FS"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 4: follow up message\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following \"\n",
        "    \"summary in the specified language:\"\n",
        "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
        ")\n",
        "# chain 4: input= summary, language and output= followup_message\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
        "                      output_key=\"followup_message\"\n",
        "                     )"
      ],
      "metadata": {
        "id": "fFpaT1JyvH7W"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# overall_chain: input= Review\n",
        "# and output= English_Review,summary, followup_message\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "AW_MiWPHx-iJ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.Review[5]\n",
        "overall_chain(review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjJmB8CW4CCZ",
        "outputId": "1957e628-279e-4a22-c1ba-2aae68709a7a"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
              " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in the store and the taste is much better... Old batch or counterfeit!?\",\n",
              " 'summary': 'The reviewer is dissatisfied with the taste and foam quality of the product and questions if it could be an old or counterfeit batch.',\n",
              " 'followup_message': \"Réponse: Nous sommes désolés d'apprendre que vous n'êtes pas satisfait de la qualité du goût et de la mousse de notre produit. Nous pouvons vous assurer que toutes nos productions respectent les normes de qualité et d'hygiène les plus strictes. Toutefois, pour vérifier si cela est dû à un lot périmé ou contrefait, nous vous proposons de nous fournir des informations sur la date d'achat et le lieu d'achat du produit. Nous ferons tout notre possible pour résoudre ce problème et pour vous offrir une expérience de dégustation optimale. Merci de votre attention.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RouterChain"
      ],
      "metadata": {
        "id": "FjBDbX7XvJjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "physics_template = \"\"\"You are a very smart physics professor. \\\n",
        "You are great at answering questions about physics in a concise\\\n",
        "and easy to understand manner. \\\n",
        "When you don't know the answer to a question you admit\\\n",
        "that you don't know.\n",
        "​\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "math_template = \"\"\"You are a very good mathematician. \\\n",
        "You are great at answering math questions. \\\n",
        "You are so good because you are able to break down \\\n",
        "hard problems into their component parts,\n",
        "answer the component parts, and then put them together\\\n",
        "to answer the broader question.\n",
        "​\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "history_template = \"\"\"You are a very good historian. \\\n",
        "You have an excellent knowledge of and understanding of people,\\\n",
        "events and contexts from a range of historical periods. \\\n",
        "You have the ability to think, reflect, debate, discuss and \\\n",
        "evaluate the past. You have a respect for historical evidence\\\n",
        "and the ability to make use of it to support your explanations \\\n",
        "and judgements.\n",
        "​\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
        "You have a passion for creativity, collaboration,\\\n",
        "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
        "understanding of theories and algorithms, and excellent communication \\\n",
        "skills. You are great at answering coding questions. \\\n",
        "You are so good because you know how to solve a problem by \\\n",
        "describing the solution in imperative steps \\\n",
        "that a machine can easily interpret and you know how to \\\n",
        "choose a solution that has a good balance between \\\n",
        "time complexity and space complexity.\n",
        "​\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "f3bpCxcDvTDv"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"physics\",\n",
        "        \"description\": \"Good for answering questions about physics\",\n",
        "        \"prompt_template\": physics_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": math_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"History\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": history_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"computer science\",\n",
        "        \"description\": \"Good for answering computer science questions\",\n",
        "        \"prompt_template\": computerscience_template\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "18aPWd_QyJI3"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ],
      "metadata": {
        "id": "JE7YE6S2vUBd"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)"
      ],
      "metadata": {
        "id": "sCY4980QyNEQ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
      ],
      "metadata": {
        "id": "bFE3jBqSyS9i"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "​\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "​\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "​\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "​\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "​\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n"
      ],
      "metadata": {
        "id": "28P7w9EAvt4B"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
      ],
      "metadata": {
        "id": "L4NNr553yXwa"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "kORSf0zNyc2w"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"What is black body radiation?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Adz4rODgv2dt",
        "outputId": "2297d41d-c6ee-4e8b-d602-a29c50a205e4"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "physics: {'input': 'Explain black body radiation'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that absorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body depends only on its temperature and is described by Planck's law. At low temperatures, the radiation is mostly in the infrared region, while at high temperatures, it shifts towards the visible and ultraviolet regions. The concept of black body radiation is important in understanding the behavior of stars, as well as in the development of technologies such as incandescent light bulbs and thermal imaging cameras.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"what is 2 + 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ot6xpODxv4BQ",
        "outputId": "b13b8dd3-8aa1-4fcb-927a-b2ea03e96576"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'what is 2 + 2'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As an AI language model, I can answer this question. The answer is 4.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "L4"
      ],
      "metadata": {
        "id": "Sy2uKpNK0jcU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52824b89-532a-4e54-87e9-1410813cd39e"
      },
      "source": [
        "# LangChain: Q&A over Documents\n",
        "\n",
        "An example might be a tool that would allow you to query a product catalog for items of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 115,
        "id": "974acf8e-8f88-42de-88f8-40a82cb58e8b"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "from IPython.display import display, Markdown\n",
        "from langchain.llms import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "height": 30,
        "id": "5bfaba30"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the raw CSV file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/anjelammcgraw/Langchain-for-LLM-Application-Development/main/OutdoorClothingCatalog_1000.csv'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open('OutdoorClothingCatalog_1000.csv', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Now you can use your CSVLoader\n",
        "file = 'OutdoorClothingCatalog_1000.csv'\n",
        "loader = CSVLoader(file_path=file)\n",
        "# Proceed with using loader as intended\n"
      ],
      "metadata": {
        "id": "xwmQuyko13zL"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "height": 30,
        "id": "9b5ab657",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0179a2ff-7fa7-47b5-e284-a0d1f0d61f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docarray in /usr/local/lib/python3.10/dist-packages (0.40.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.25.2)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from docarray) (3.10.1)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from docarray) (2.7.0)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (13.7.1)\n",
            "Requirement already satisfied: types-requests>=2.28.11.6 in /usr/local/lib/python3.10/dist-packages (from docarray) (2.31.0.20240406)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (4.11.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (2.16.1)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.10/dist-packages (from types-requests>=2.28.11.6->docarray) (2.0.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install docarray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "height": 64,
        "id": "9e200726"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "height": 47,
        "id": "34562d81"
      },
      "outputs": [],
      "source": [
        "query =\"Please list all your shirts with sun protection \\\n",
        "in a table in markdown and summarize each one.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "height": 98,
        "id": "cfd0cc37"
      },
      "outputs": [],
      "source": [
        "llm_replacement_model = OpenAI(temperature=0,\n",
        "                               model='gpt-3.5-turbo-instruct')\n",
        "\n",
        "response = index.query(query,\n",
        "                       llm = llm_replacement_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "height": 30,
        "scrolled": true,
        "id": "ae21f1ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "5a635acb-2cde-42d7-c77f-3792eadfdae9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "\n\n| Name | Description | Sun Protection Rating |\n| --- | --- | --- |\n| Men's Tropical Plaid Short-Sleeve Shirt | Made of 100% polyester, UPF 50+ rated, wrinkle-resistant, front and back cape venting, two front bellows pockets, imported | SPF 50+, blocks 98% of harmful UV rays |\n| Men's Plaid Tropic Shirt, Short-Sleeve | Made of 52% polyester and 48% nylon, UPF 50+ rated, SunSmart technology, wrinkle-free, front and back cape venting, two front bellows pockets, imported | SPF 50+, blocks 98% of harmful UV rays |\n| Men's TropicVibe Shirt, Short-Sleeve | Made of 71% nylon and 29% polyester, UPF 50+ rated, wrinkle-resistant, front and back cape venting, two front bellows pockets, imported | SPF 50+, blocks 98% of harmful UV rays |\n| Sun Shield Shirt | Made of 78% nylon and 22% Lycra Xtra Life fiber, UPF 50+ rated, moisture-wicking, fits comfortably over swimsuit, abrasion-resistant, imported | SPF"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2534597e-4b0c-4563-a208-e2dd91064438"
      },
      "source": [
        "## Step By Step"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from langchain.document_loaders import CSVLoader\n",
        "\n",
        "# URL of the raw CSV file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/anjelammcgraw/Langchain-for-LLM-Application-Development/main/OutdoorClothingCatalog_1000.csv'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    # Assuming you want to save the file temporarily in the current working directory\n",
        "    with open('OutdoorClothingCatalog_1000.csv', 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Use CSVLoader to load the downloaded file\n",
        "    loader = CSVLoader(file_path='OutdoorClothingCatalog_1000.csv')\n",
        "    docs = loader.load()\n",
        "\n",
        "    # Example usage of the loaded documents\n",
        "    print(docs[0])\n",
        "else:\n",
        "    print(\"Failed to download the file. Status Code:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBb89fYG5l48",
        "outputId": "403552ed-440e-48e8-986f-13c9c6d931b4"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content=\": 0\\nname: Women's Campside Oxfords\\ndescription: This ultracomfortable lace-to-toe Oxford boasts a super-soft canvas, thick cushioning, and quality construction for a broken-in feel from the first time you put them on. \\n\\nSize & Fit: Order regular shoe size. For half sizes not offered, order up to next whole size. \\n\\nSpecs: Approx. weight: 1 lb.1 oz. per pair. \\n\\nConstruction: Soft canvas material for a broken-in feel and look. Comfortable EVA innersole with Cleansport NXT® antimicrobial odor control. Vintage hunt, fish and camping motif on innersole. Moderate arch contour of innersole. EVA foam midsole for cushioning and support. Chain-tread-inspired molded rubber outsole with modified chain-tread pattern. Imported. \\n\\nQuestions? Please contact us for any inquiries.\" metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "height": 47,
        "id": "e875693a"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "height": 30,
        "id": "779bec75"
      },
      "outputs": [],
      "source": [
        "embed = embeddings.embed_query(\"Hi my name is Harrison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "height": 30,
        "id": "699aaaf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1f73933-66e2-4f46-bf9e-9fc0c3d81677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1536\n"
          ]
        }
      ],
      "source": [
        "print(len(embed))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "height": 30,
        "id": "9d00d346",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cf19b00-cf4f-4899-a71a-d43b383cac19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.0219938028518557, 0.006747527976699378, -0.018252847709138532, -0.03916704653175717, -0.013997197145759583]\n"
          ]
        }
      ],
      "source": [
        "print(embed[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "height": 81,
        "id": "27ad0bb0"
      },
      "outputs": [],
      "source": [
        "db = DocArrayInMemorySearch.from_documents(\n",
        "    docs,\n",
        "    embeddings\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "height": 30,
        "id": "0329bfd5"
      },
      "outputs": [],
      "source": [
        "query = \"Please suggest a shirt with sunblocking\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "height": 30,
        "id": "7909c6b7"
      },
      "outputs": [],
      "source": [
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "height": 30,
        "id": "43321853",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4485e237-1fee-4f3c-d77a-7e994a78d650"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 163
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "height": 30,
        "id": "6eba90b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b6ea9d-d9c4-4f31-c9eb-40adce72ee26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content=': 255\\nname: Sun Shield Shirt by\\ndescription: \"Block the sun, not the fun – our high-performance sun shirt is guaranteed to protect from harmful UV rays. \\n\\nSize & Fit: Slightly Fitted: Softly shapes the body. Falls at hip.\\n\\nFabric & Care: 78% nylon, 22% Lycra Xtra Life fiber. UPF 50+ rated – the highest rated sun protection possible. Handwash, line dry.\\n\\nAdditional Features: Wicks moisture for quick-drying comfort. Fits comfortably over your favorite swimsuit. Abrasion resistant for season after season of wear. Imported.\\n\\nSun Protection That Won\\'t Wear Off\\nOur high-performance fabric provides SPF 50+ sun protection, blocking 98% of the sun\\'s harmful rays. This fabric is recommended by The Skin Cancer Foundation as an effective UV protectant.', metadata={'source': 'OutdoorClothingCatalog_1000.csv', 'row': 255})"
            ]
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "docs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "height": 30,
        "id": "c0c3596e"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "height": 30,
        "id": "0625f5e8"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "height": 47,
        "id": "a573f58a"
      },
      "outputs": [],
      "source": [
        "qdocs = \"\".join([docs[i].page_content for i in range(len(docs))])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "height": 64,
        "id": "14682d95"
      },
      "outputs": [],
      "source": [
        "response = llm.call_as_llm(f\"{qdocs} Question: Please list all your \\\n",
        "shirts with sun protection in a table in markdown and summarize each one.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "height": 30,
        "id": "8bba545b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "outputId": "facd9111-56e4-4ee6-dc9a-b1d76603674b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Name | Description |\n| --- | --- |\n| Sun Shield Shirt | High-performance sun shirt with UPF 50+ sun protection, moisture-wicking, and abrasion-resistant fabric. Recommended by The Skin Cancer Foundation. |\n| Men's Plaid Tropic Shirt | Ultracomfortable shirt with UPF 50+ sun protection, wrinkle-free fabric, and front/back cape venting. Made with 52% polyester and 48% nylon. |\n| Men's TropicVibe Shirt | Men's sun-protection shirt with built-in UPF 50+ and front/back cape venting. Made with 71% nylon and 29% polyester. |\n| Men's Tropical Plaid Short-Sleeve Shirt | Lightest hot-weather shirt with UPF 50+ sun protection, front/back cape venting, and two front bellows pockets. Made with 100% polyester. |\n\nAll of these shirts provide UPF 50+ sun protection, blocking 98% of the sun's harmful rays. They also have additional features such as moisture-wicking, wrinkle-free fabric, and front/back cape venting for added comfort."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "height": 115,
        "id": "32c94d22"
      },
      "outputs": [],
      "source": [
        "qa_stuff = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "height": 47,
        "id": "e4769316"
      },
      "outputs": [],
      "source": [
        "query =  \"Please list all your shirts with sun protection in a table \\\n",
        "in markdown and summarize each one.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "height": 30,
        "id": "1fc3c2f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fbf8c34-93d9-4621-ec93-c830c3e65b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = qa_stuff.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "height": 30,
        "id": "fba1a5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "6c762ae8-1278-4503-998e-a27063f66bb5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "| Shirt Number | Name | Description |\n| --- | --- | --- |\n| 618 | Men's Tropical Plaid Short-Sleeve Shirt | Rated UPF 50+ for superior protection from the sun's UV rays. Made of 100% polyester and is wrinkle-resistant. With front and back cape venting that lets in cool breezes and two front bellows pockets. |\n| 374 | Men's Plaid Tropic Shirt, Short-Sleeve | Rated to UPF 50+ and offers sun protection. Made with 52% polyester and 48% nylon, this shirt is machine washable and dryable. Additional features include front and back cape venting, two front bellows pockets. |\n| 535 | Men's TropicVibe Shirt, Short-Sleeve | Built-in UPF 50+ has the lightweight feel you want and the coverage you need when the air is hot and the UV rays are strong. Made with 71% Nylon, 29% Polyester. Wrinkle-resistant. Front and back cape venting lets in cool breezes. Two front bellows pockets. |\n| 255 | Sun Shield Shirt | High-performance sun shirt is guaranteed to protect from harmful UV rays. Made with 78% nylon, 22% Lycra Xtra Life fiber. Fits comfortably over your favorite swimsuit. Abrasion-resistant. |\n\nThe Men's Tropical Plaid Short-Sleeve Shirt is made of 100% polyester and is wrinkle-resistant. It is rated UPF 50+ for superior protection from the sun's UV rays. The Men's Plaid Tropic Shirt, Short-Sleeve is made with 52% polyester and 48% nylon, and is rated to UPF 50+. The Men's TropicVibe Shirt, Short-Sleeve has built-in UPF 50+ and is made with 71% Nylon, 29% Polyester. The Sun Shield Shirt is made with 78% nylon, 22% Lycra Xtra Life fiber and is guaranteed to protect from harmful UV rays."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "height": 30,
        "id": "500ec062"
      },
      "outputs": [],
      "source": [
        "response = index.query(query, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "height": 81,
        "id": "2cffb19f"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch,\n",
        "    embedding=embeddings,\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "L5"
      ],
      "metadata": {
        "id": "o4Av7_ut6KrJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92N_U8P6Mkm"
      },
      "source": [
        "# LangChain: Evaluation\n",
        "\n",
        "## Outline:\n",
        "\n",
        "* Example generation\n",
        "* Manual evaluation (and debuging)\n",
        "* LLM-assisted evaluation\n",
        "* LangChain evaluation platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28008949"
      },
      "source": [
        "## Create our QandA application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "height": 98,
        "id": "cjIXQ1BQ6Mkp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import CSVLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.vectorstores import DocArrayInMemorySearch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# URL of the raw CSV file on GitHub\n",
        "url = 'https://raw.githubusercontent.com/anjelammcgraw/Langchain-for-LLM-Application-Development/main/OutdoorClothingCatalog_1000.csv'\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "with open('OutdoorClothingCatalog_1000.csv', 'wb') as f:\n",
        "    f.write(response.content)\n",
        "\n",
        "# Now you can use your CSVLoader\n",
        "file = 'OutdoorClothingCatalog_1000.csv'\n",
        "loader = CSVLoader(file_path=file)\n",
        "# Proceed with using loader as intended\n"
      ],
      "metadata": {
        "id": "HdMzngSu6rW0"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "height": 64,
        "id": "Ph0wDTsn6rW4"
      },
      "outputs": [],
      "source": [
        "index = VectorstoreIndexCreator(\n",
        "    vectorstore_cls=DocArrayInMemorySearch\n",
        ").from_loaders([loader])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "height": 183,
        "id": "a2006054"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature = 0.0, model=llm_model)\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=index.vectorstore.as_retriever(),\n",
        "    verbose=True,\n",
        "    chain_type_kwargs = {\n",
        "        \"document_separator\": \"<<<<>>>>>\"\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "791ebd73"
      },
      "source": [
        "### Coming up with test datapoints"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt0ykdE_61GI",
        "outputId": "235ed1b0-95a2-469f-8b83-10e4458a8059"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d548aef"
      },
      "source": [
        "### Hard-coded examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "height": 217,
        "id": "c2d59bf2"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
        "        have side pockets?\",\n",
        "        \"answer\": \"Yes\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What collection is the Ultra-Lofty \\\n",
        "        850 Stretch Down Hooded Jacket from?\",\n",
        "        \"answer\": \"The DownTek collection\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ce3e4f"
      },
      "source": [
        "### LLM-Generated examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "height": 47,
        "id": "d44f8376"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAGenerateChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "height": 30,
        "id": "34e87816"
      },
      "outputs": [],
      "source": [
        "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "height": 30,
        "id": "31fb5bd0-fc2a-478c-9a09-6fe7e3307241"
      },
      "outputs": [],
      "source": [
        "# the warning below can be safely ignored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "height": 64,
        "id": "62abae09"
      },
      "outputs": [],
      "source": [
        "new_examples = example_gen_chain.apply_and_parse(\n",
        "    [{\"doc\": t} for t in data[:5]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "height": 30,
        "id": "97ab28b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3abcf4d9-1697-423c-eeec-098bea55d7f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'qa_pairs': {'query': 'What is the subject of the document?',\n",
              "  'answer': 'The subject of the document is \"Product\".'}}"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ],
      "source": [
        "new_examples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "height": 30,
        "id": "0ebe4228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "b2108b2b-5460-44d4-d51a-ea51acff2daf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   Product                                             Review\n",
              "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
              "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
              "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
              "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
              "4  Milk Frother Handheld\\n   I loved this product. But they only seem to l..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9c4b9fcf-5a84-475e-8966-9735396242cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen Size Sheet Set</td>\n",
              "      <td>I ordered a king size set. My only criticism w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Waterproof Phone Pouch</td>\n",
              "      <td>I loved the waterproof sac, although the openi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Luxury Air Mattress</td>\n",
              "      <td>This mattress had a small hole in the top of i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pillows Insert</td>\n",
              "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Milk Frother Handheld\\n</td>\n",
              "      <td>I loved this product. But they only seem to l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c4b9fcf-5a84-475e-8966-9735396242cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c4b9fcf-5a84-475e-8966-9735396242cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c4b9fcf-5a84-475e-8966-9735396242cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d2b30e0b-00f7-4d14-9030-3a177878e7ed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2b30e0b-00f7-4d14-9030-3a177878e7ed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d2b30e0b-00f7-4d14-9030-3a177878e7ed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"data[:5]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Waterproof Phone Pouch\",\n          \"Milk Frother Handheld\\n\",\n          \"Luxury Air Mattress\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"I loved the waterproof sac, although the opening was made of a hard plastic. I don\\u2019t know if that would break easily. But I couldn\\u2019t turn my phone on, once it was in the pouch.\",\n          \"\\u00a0I loved this product. But they only seem to last a few months. The company was great replacing the first one (the frother falls out of the handle and can't be fixed). The after 4 months my second one did the same. I only use the frother for coffee once a day. It's not overuse or abuse. I'm very disappointed and will look for another. As I understand they will only replace once. Anyway, if you have one good luck.\",\n          \"This mattress had a small hole in the top of it (took forever to find where it was), and the patches that they provide did not work, maybe because it's the top of the mattress where it's kind of like fabric and a patch won't stick. Maybe I got unlucky with a defective mattress, but where's quality assurance for this company? That flat out should not happen. Emphasis on flat. Cause that's what the mattress was. Seriously horrible experience, ruined my friend's stay with me. Then they make you ship it back instead of just providing a refund, which is also super annoying to pack up an air mattress and take it to the UPS store. This company is the worst, and this mattress is the worst.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ],
      "source": [
        "data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faf25f2f"
      },
      "source": [
        "### Combine examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "height": 30,
        "id": "ada2a3fc"
      },
      "outputs": [],
      "source": [
        "examples += new_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "height": 30,
        "id": "9cdf5cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "911fa680-bb51-4423-f52c-00674dc337b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, the Cozy Comfort Pullover Set has side pockets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 194
        }
      ],
      "source": [
        "qa.run(examples[0][\"query\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63f3cb08"
      },
      "source": [
        "## Manual Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "height": 47,
        "id": "fcaf622e"
      },
      "outputs": [],
      "source": [
        "import langchain\n",
        "langchain.debug = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "height": 30,
        "id": "8a142638",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ee65ec9-6b39-487d-d313-820cc85b0b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"query\": \"Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] Entering Chain run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"Do the Cozy Comfort Pullover Set        have side pockets?\",\n",
            "  \"context\": \": 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece – made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n – Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece – made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n – Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\\nHuman: Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [351ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Yes, the Cozy Comfort Pullover Set has side pockets.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Yes, the Cozy Comfort Pullover Set has side pockets.\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 13,\n",
            "      \"prompt_tokens\": 735,\n",
            "      \"total_tokens\": 748\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain > 4:chain:LLMChain] [354ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Yes, the Cozy Comfort Pullover Set has side pockets.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 3:chain:StuffDocumentsChain] [357ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output_text\": \"Yes, the Cozy Comfort Pullover Set has side pockets.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [560ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"result\": \"Yes, the Cozy Comfort Pullover Set has side pockets.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, the Cozy Comfort Pullover Set has side pockets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 196
        }
      ],
      "source": [
        "qa.run(examples[0][\"query\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "height": 47,
        "id": "b3d6bef0"
      },
      "outputs": [],
      "source": [
        "# Turn off the debug mode\n",
        "langchain.debug = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5bdbdce"
      },
      "source": [
        "## LLM assisted evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
        "        have side pockets?\",\n",
        "        \"answer\": \"Yes\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What collection is the Ultra-Lofty \\\n",
        "        850 Stretch Down Hooded Jacket from?\",\n",
        "        \"answer\": \"The DownTek collection\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "4DX_SAwS8Md0"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "height": 30,
        "id": "a4dca05a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4425148b-72f0-4f92-80bb-ac4ce2c209a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "predictions = qa.apply(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "height": 30,
        "id": "6012a3e0"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation.qa import QAEvalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "height": 47,
        "id": "724b1c0b"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
        "eval_chain = QAEvalChain.from_llm(llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "height": 30,
        "id": "8b46ae55"
      },
      "outputs": [],
      "source": [
        "graded_outputs = eval_chain.evaluate(examples, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, output in enumerate(graded_outputs):\n",
        "    print(f\"Graded Output {i}: {output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_Mp3Vql8mFI",
        "outputId": "40a5a117-c8c6-4141-c66f-df64c9ecd38c"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graded Output 0: {'results': 'CORRECT'}\n",
            "Graded Output 1: {'results': 'CORRECT'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, eg in enumerate(examples):\n",
        "    print(f\"Example {i}:\")\n",
        "    print(\"Question: \" + predictions[i]['query'])\n",
        "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
        "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
        "    # Use get to provide a default value if 'text' key is missing\n",
        "    print(\"Predicted Grade: \" + graded_outputs[i].get('text', 'No grade available'))\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mRKTSUv8pwI",
        "outputId": "d4527277-78ee-45c3-ed77-ae6d19744c9d"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 0:\n",
            "Question: Do the Cozy Comfort Pullover Set        have side pockets?\n",
            "Real Answer: Yes\n",
            "Predicted Answer: Yes, the Cozy Comfort Pullover Set has side pockets.\n",
            "Predicted Grade: No grade available\n",
            "\n",
            "Example 1:\n",
            "Question: What collection is the Ultra-Lofty         850 Stretch Down Hooded Jacket from?\n",
            "Real Answer: The DownTek collection\n",
            "Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n",
            "Predicted Grade: No grade available\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "height": 30,
        "id": "d95133bb-43b6-441d-9ba5-00ef5609ccc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "075e7841-71e9-4399-9670-7045e283aea3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'results': 'CORRECT'}"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "graded_outputs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fad0ddd1"
      },
      "source": [
        "## LangChain evaluation platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef63bb24"
      },
      "source": [
        "The LangChain evaluation platform, LangChain Plus, can be accessed here https://www.langchain.plus/.  \n",
        "Use the invite code `lang_learners_2023`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "L6\n"
      ],
      "metadata": {
        "id": "9QGGodBJ7h1S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-cPbxZv8z20"
      },
      "source": [
        "# LangChain: Agents\n",
        "\n",
        "## Outline:\n",
        "\n",
        "* Using built in LangChain tools: DuckDuckGo search and Wikipedia\n",
        "* Defining your own tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e3b9ad2"
      },
      "source": [
        "## Built-in LangChain tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "height": 30,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG2Q6xUp8z25",
        "outputId": "dcf2c9f8-942b-4fdf-93d1-00f9aa0efa64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=46494fc0c01e2ffab68d744bc18572807388b124060e2a3f4cd58e8bb6409e99\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "from langchain.schema import HumanMessage, SystemMessage, ChatMessage, AIMessage"
      ],
      "metadata": {
        "id": "QCejRNW6A0v3"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "7b93d0e8"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "7aadd5c1"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(temperature=0, model=llm_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "394924ac"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import load_tools\n",
        "\n",
        "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "02cd4956"
      },
      "outputs": [],
      "source": [
        "agent= initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "height": 30,
        "tags": [],
        "id": "44601fe4-5f24-4ff2-a5df-a7e194a5b4f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c21d676b-4f27-41b4-8730-3cd50eedaf53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use a calculator to find the answer to this math problem.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"300*0.25\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have successfully calculated 25% of 300 using the calculator tool. \n",
            "\n",
            "Final Answer: 25% of 300 is 75.0.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the 25% of 300?', 'output': '25% of 300 is 75.0.'}"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ],
      "source": [
        "agent(\"What is the 25% of 300?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ee3df6-ded9-4c96-ba47-a6cf636c1b9b"
      },
      "source": [
        "## Wikipedia example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "441df699-0a52-4812-a0a7-4605204f0aa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bc8b83-09d5-4b08-ca0b-82bc641a50a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I should use the wikipedia tool to search for Tom M. Mitchell's bibliography.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"wikipedia\",\n",
            "  \"action_input\": \"Tom M. Mitchell\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: Tom M. Mitchell\n",
            "Summary: Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and the Founders University Professor at Carnegie Mellon University (CMU). He is a founder and former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Academy of Arts and Sciences, the American Association for the Advancement of Science and a Fellow and past President of the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.\n",
            "\n",
            "Page: Tom Mitchell (Australian footballer)\n",
            "Summary: Thomas Mitchell (born 31 May 1993) is a professional Australian rules footballer playing for the Collingwood Football Club in the Australian Football League (AFL). He previously played for the Sydney Swans from 2012 to 2016, and the Hawthorn Football Club between 2017 and 2022. Mitchell won the Brownlow Medal as the league's best and fairest player in 2018 and set the record for the most disposals in a VFL/AFL match, accruing 54 in a game against Collingwood during that season. Whom he would later join in 2023, en route to winning the 2023 AFL Grand Final and his first AFL premiership.\n",
            "\n",
            "\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mAccording to the wikipedia page, Tom M. Mitchell is the author of the textbook \"Machine Learning\".\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"wikipedia\",\n",
            "  \"action_input\": \"Machine Learning (book)\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mPage: Machine learning\n",
            "Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Recently, artificial neural networks have been able to surpass many previous approaches in performance.\n",
            "ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. When applied to business problems, it is known under the name predictive analytics. Although not all machine learning is statistically based, computational statistics is an important source of the field's methods.\n",
            "The mathematical foundations of ML are provided by mathematical optimization (mathematical programming) methods. Data mining is a related (parallel) field of study, focusing on exploratory data analysis (EDA) through unsupervised learning. \n",
            "From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n",
            "\n",
            "Page: Quantum machine learning\n",
            "Summary: Quantum machine learning is the integration of quantum algorithms within machine learning programs.\n",
            "The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data.\n",
            "Beyond quantum computing, the term \"quantum machine learning\" is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments.\n",
            "Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa.\n",
            "Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n",
            "\n",
            "Page: Timeline of machine learning\n",
            "Summary: This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events in machine learning are included.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mTom M. Mitchell is the author of the textbook \"Machine Learning\".\n",
            "\n",
            "Final Answer: \"Machine Learning\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "question = \"Tom M. Mitchell is an American computer scientist \\\n",
        "and the Founders University Professor at Carnegie Mellon University (CMU)\\\n",
        "what book did he write?\"\n",
        "result = agent(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90299a00-eb79-4755-bb4b-bf61d88c7fed"
      },
      "source": [
        "## Python Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {
        "height": 149,
        "tags": [],
        "id": "d05e7dc6-f07e-43df-a2f6-9a0816291861"
      },
      "outputs": [],
      "source": [
        "customer_list = [[\"Harrison\", \"Chase\"],\n",
        "                 [\"Lang\", \"Chain\"],\n",
        "                 [\"Dolly\", \"Too\"],\n",
        "                 [\"Elle\", \"Elem\"],\n",
        "                 [\"Geoff\",\"Fusion\"],\n",
        "                 [\"Trance\",\"Former\"],\n",
        "                 [\"Jen\",\"Ayai\"]\n",
        "                ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599bkRJjGGUK",
        "outputId": "943f86a9-3aba-4a4f-8c50-7f3de56acaa2"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Jen', 'Ayai'],\n",
              " ['Lang', 'Chain'],\n",
              " ['Harrison', 'Chase'],\n",
              " ['Elle', 'Elem'],\n",
              " ['Trance', 'Former'],\n",
              " ['Geoff', 'Fusion'],\n",
              " ['Dolly', 'Too']]"
            ]
          },
          "metadata": {},
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customer_list.sort(key=lambda x: (x[1], x[0]))"
      ],
      "metadata": {
        "id": "Fz5vLLMIGPMl"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "height": 64,
        "tags": [],
        "id": "f887f8f9-216e-489a-945a-f38fbccbe5c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b8914c53-439c-4bba-a222-b3ddcae1bc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [694ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Question: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Question: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 54,\n",
            "      \"prompt_tokens\": 365,\n",
            "      \"total_tokens\": 419\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [697ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Question: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:wikipedia] Entering Tool run with input:\n",
            "\u001b[0m\"second customer in the list python\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:wikipedia] [2.09s] Exiting Tool run with output:\n",
            "\u001b[0m\"Page: Python (programming language)\n",
            "Summary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
            "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
            "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\n",
            "Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n",
            "\n",
            "\n",
            "\n",
            "Page: Monty Python\n",
            "Summary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \"an important moment in the evolution of television comedy\".\n",
            "Monty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\n",
            "At the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\n",
            "\n",
            "Page: Monty Python's The Meaning of Life\n",
            "Summary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\n",
            "Unlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\n",
            "Released on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "  \"agent_scratchpad\": \"This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [626ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 44,\n",
            "      \"prompt_tokens\": 1294,\n",
            "      \"total_tokens\": 1338\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [629ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 7:tool:wikipedia] Entering Tool run with input:\n",
            "\u001b[0m\"indexing python\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 7:tool:wikipedia] [1.95s] Exiting Tool run with output:\n",
            "\u001b[0m\"Page: Python Package Index\n",
            "Summary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \"Cheese Shop\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \n",
            "As of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\n",
            "In May 2023, more than 450 000 Python packages are available.\n",
            "PyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \"wheels.\"\n",
            "PyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\n",
            "\n",
            "\n",
            "\n",
            "Page: Python (programming language)\n",
            "Summary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\n",
            "Python is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.\n",
            "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\n",
            "Python consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\n",
            "\n",
            "\n",
            "\n",
            "Page: Monty Python\n",
            "Summary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \"an important moment in the evolution of television comedy\".\n",
            "Monty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\n",
            "At the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 8:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "  \"agent_scratchpad\": \"This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 8:chain:LLMChain > 9:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 8:chain:LLMChain > 9:llm:ChatOpenAI] [652ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 43,\n",
            "      \"prompt_tokens\": 2202,\n",
            "      \"total_tokens\": 2245\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 8:chain:LLMChain] [655ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator] Entering Tool run with input:\n",
            "\u001b[0m\"2-1\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"2-1\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain > 12:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"question\": \"2-1\",\n",
            "  \"stop\": [\n",
            "    \"```output\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain > 12:chain:LLMChain > 13:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${Question with math problem.}\\n```text\\n${single line mathematical expression that solves the problem}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${Output of running the code}\\n```\\nAnswer: ${Answer}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\\\"37593 * 67\\\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: 2-1\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain > 12:chain:LLMChain > 13:llm:ChatOpenAI] [392ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"```text\\n2-1\\n```\\n...numexpr.evaluate(\\\"2-1\\\")...\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"```text\\n2-1\\n```\\n...numexpr.evaluate(\\\"2-1\\\")...\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 19,\n",
            "      \"prompt_tokens\": 204,\n",
            "      \"total_tokens\": 223\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain > 12:chain:LLMChain] [395ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"```text\\n2-1\\n```\\n...numexpr.evaluate(\\\"2-1\\\")...\\n\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator > 11:chain:LLMMathChain] [399ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"answer\": \"Answer: 1\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 10:tool:Calculator] [401ms] Exiting Tool run with output:\n",
            "\u001b[0m\"Answer: 1\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 14:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "  \"agent_scratchpad\": \"This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\\nObservation: Answer: 1\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 14:chain:LLMChain > 15:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\\nObservation: Answer: 1\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 14:chain:LLMChain > 15:llm:ChatOpenAI] [807ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I can use the answer from the previous action to access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"sorted customers python\\\"\\n}\\n```\\n\\n\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I can use the answer from the previous action to access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"sorted customers python\\\"\\n}\\n```\\n\\n\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 42,\n",
            "      \"prompt_tokens\": 2254,\n",
            "      \"total_tokens\": 2296\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 14:chain:LLMChain] [811ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"I can use the answer from the previous action to access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"sorted customers python\\\"\\n}\\n```\\n\\n\\n\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 16:tool:wikipedia] Entering Tool run with input:\n",
            "\u001b[0m\"sorted customers python\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 16:tool:wikipedia] [2.01s] Exiting Tool run with output:\n",
            "\u001b[0m\"Page: Monty Python\n",
            "Summary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \"an important moment in the evolution of television comedy\".\n",
            "Monty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\n",
            "At the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\n",
            "\n",
            "Page: Monty Python's Flying Circus\n",
            "Summary: Monty Python's Flying Circus (also known as simply Monty Python) is a British surreal sketch comedy series created by and starring Graham Chapman, John Cleese, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam, who became known collectively as \"Monty Python\", or the \"Pythons\". The first episode was recorded at the BBC on 7 September 1969 and premiered on 5 October on BBC1, with 45 episodes airing over four series from 1969 to 1974, plus two episodes for German TV. A feature film adaptation of several sketches, And Now for Something Completely Different, was released in 1971.\n",
            "The series stands out for its use of absurd situations, mixed with risqué and innuendo-laden humour, sight gags, and observational sketches without punchlines. Live-action segments were broken up with animations by Gilliam, often merging with the live action to form segues. The overall format used for the series followed and elaborated upon the style used by Spike Milligan in his groundbreaking series Q..., rather than the traditional sketch show format. The Pythons play the majority of the series's characters themselves, along with supporting cast members including Carol Cleveland (referred to by the team as the unofficial \"Seventh Python\"), Connie Booth (Cleese's first wife), series producer Ian MacNaughton, Ian Davidson, musician Neil Innes, and Fred Tomlinson and the Fred Tomlinson Singers for musical numbers.\n",
            "The programme came about as the six Pythons, having met each other through university and in various radio and television programmes in the 1960s, sought to make a new sketch comedy show unlike anything else on British television at the time. Much of the humour in the series' various episodes and sketches targets the idiosyncrasies of British life, especially that of professionals, as well as aspects of politics. Their comedy is often pointedly intellectual, with numerous erudite references to philosophers and literary figures and their works. The team intended their humour to be impossible to categorise, and succeeded so completely that the adjective \"Pythonesque\" was invented to defi\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 17:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\",\n",
            "  \"agent_scratchpad\": \"This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\\nObservation: Answer: 1\\nThought:I can use the answer from the previous action to access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"sorted customers python\\\"\\n}\\n```\\n\\n\\n\\nObservation: Page: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's Flying Circus\\nSummary: Monty Python's Flying Circus (also known as simply Monty Python) is a British surreal sketch comedy series created by and starring Graham Chapman, John Cleese, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam, who became known collectively as \\\"Monty Python\\\", or the \\\"Pythons\\\". The first episode was recorded at the BBC on 7 September 1969 and premiered on 5 October on BBC1, with 45 episodes airing over four series from 1969 to 1974, plus two episodes for German TV. A feature film adaptation of several sketches, And Now for Something Completely Different, was released in 1971.\\nThe series stands out for its use of absurd situations, mixed with risqué and innuendo-laden humour, sight gags, and observational sketches without punchlines. Live-action segments were broken up with animations by Gilliam, often merging with the live action to form segues. The overall format used for the series followed and elaborated upon the style used by Spike Milligan in his groundbreaking series Q..., rather than the traditional sketch show format. The Pythons play the majority of the series's characters themselves, along with supporting cast members including Carol Cleveland (referred to by the team as the unofficial \\\"Seventh Python\\\"), Connie Booth (Cleese's first wife), series producer Ian MacNaughton, Ian Davidson, musician Neil Innes, and Fred Tomlinson and the Fred Tomlinson Singers for musical numbers.\\nThe programme came about as the six Pythons, having met each other through university and in various radio and television programmes in the 1960s, sought to make a new sketch comedy show unlike anything else on British television at the time. Much of the humour in the series' various episodes and sketches targets the idiosyncrasies of British life, especially that of professionals, as well as aspects of politics. Their comedy is often pointedly intellectual, with numerous erudite references to philosophers and literary figures and their works. The team intended their humour to be impossible to categorise, and succeeded so completely that the adjective \\\"Pythonesque\\\" was invented to defi\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 17:chain:LLMChain > 18:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: Here are the sorted customers: [['Jen', 'Ayai'], ['Lang', 'Chain'], ['Harrison', 'Chase'], ['Elle', 'Elem'], ['Trance', 'Former'], ['Geoff', 'Fusion'], ['Dolly', 'Too']]\\n\\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: What is the first name of the second customer in the list?\\nThought: I can access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"second customer in the list python\\\"\\n}\\n```\\n\\nObservation: Page: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's The Meaning of Life\\nSummary: Monty Python's The Meaning of Life, also known simply as The Meaning of Life, is a 1983 British musical sketch comedy film written and performed by the Monty Python troupe, directed by Terry Jones. The Meaning of Life was the last feature film to star all six Python members before the death of Graham Chapman in 1989.\\nUnlike Holy Grail and Life of Brian, the film's two predecessors, which each told a single, more-or-less coherent story, The Meaning of Life returned to the sketch format of the troupe's original television series and their first film from twelve years earlier, And Now for Something Completely Different, loosely structured as a series of comic sketches about the various stages of life. It was accompanied by the short film The Crimson Permanent Assurance.\\nReleased on 23 June 1983 in the United Kingdom, The Meaning of Life was not as acclaimed as its predecessors, but was still well received critically and was a minor box office success; the film grossed almost $43 million against a $9 million budget. It was screened at the 1983 Cannes Film Festival, where it won the Grand Prix. The f\\nThought:I need to access the list using indexing to get the first name of the second customer\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"indexing python\\\"\\n}\\n```\\n\\n\\nObservation: Page: Python Package Index\\nSummary: The Python Package Index, abbreviated as PyPI () and also known as the Cheese Shop (a reference to the Monty Python's Flying Circus sketch \\\"Cheese Shop\\\"),: 8 : 742  is the official third-party software repository for Python. It is analogous to the CPAN repository for Perl: 36  and to the CRAN repository for R. PyPI is run by the Python Software Foundation, a charity. Some package managers, including pip, use PyPI as the default source for packages and their dependencies.  \\nAs of 17 January 2022, more than 350,000 Python packages can be accessed through PyPI.\\nIn May 2023, more than 450 000 Python packages are available.\\nPyPI primarily hosts Python packages in the form of archives called sdists (source distributions) or precompiled \\\"wheels.\\\"\\nPyPI as an index allows users to search for packages by keywords or by filters against their metadata, such as free software license or compatibility with POSIX. A single entry on PyPI is able to store, aside from just a package and its metadata, previous releases of the package, precompiled wheels (e.g. containing DLLs on Windows), as well as different forms for different operating systems and Python versions.\\n\\n\\n\\nPage: Python (programming language)\\nSummary: Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.\\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \\\"batteries included\\\" language due to its comprehensive standard library.\\nGuido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000. Python 3.0, released in 2008, was a major revision not completely backward-compatible with earlier versions. Python 2.7.18, released in 2020, was the last release of Python 2.\\nPython consistently ranks as one of the most popular programming languages, and has gained widespread use in the machine learning community.\\n\\n\\n\\nPage: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of\\nThought:I need to use indexing to get the first name of the second customer in the list\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"Calculator\\\",\\n  \\\"action_input\\\": \\\"2-1\\\"\\n}\\n```\\n\\n\\nObservation: Answer: 1\\nThought:I can use the answer from the previous action to access the list using indexing\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"wikipedia\\\",\\n  \\\"action_input\\\": \\\"sorted customers python\\\"\\n}\\n```\\n\\n\\n\\nObservation: Page: Monty Python\\nSummary: Monty Python (also collectively known as the Pythons) were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music. Their sketch show has been called \\\"an important moment in the evolution of television comedy\\\".\\nMonty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content. A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy. They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.\\nAt the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.\\n\\nPage: Monty Python's Flying Circus\\nSummary: Monty Python's Flying Circus (also known as simply Monty Python) is a British surreal sketch comedy series created by and starring Graham Chapman, John Cleese, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam, who became known collectively as \\\"Monty Python\\\", or the \\\"Pythons\\\". The first episode was recorded at the BBC on 7 September 1969 and premiered on 5 October on BBC1, with 45 episodes airing over four series from 1969 to 1974, plus two episodes for German TV. A feature film adaptation of several sketches, And Now for Something Completely Different, was released in 1971.\\nThe series stands out for its use of absurd situations, mixed with risqué and innuendo-laden humour, sight gags, and observational sketches without punchlines. Live-action segments were broken up with animations by Gilliam, often merging with the live action to form segues. The overall format used for the series followed and elaborated upon the style used by Spike Milligan in his groundbreaking series Q..., rather than the traditional sketch show format. The Pythons play the majority of the series's characters themselves, along with supporting cast members including Carol Cleveland (referred to by the team as the unofficial \\\"Seventh Python\\\"), Connie Booth (Cleese's first wife), series producer Ian MacNaughton, Ian Davidson, musician Neil Innes, and Fred Tomlinson and the Fred Tomlinson Singers for musical numbers.\\nThe programme came about as the six Pythons, having met each other through university and in various radio and television programmes in the 1960s, sought to make a new sketch comedy show unlike anything else on British television at the time. Much of the humour in the series' various episodes and sketches targets the idiosyncrasies of British life, especially that of professionals, as well as aspects of politics. Their comedy is often pointedly intellectual, with numerous erudite references to philosophers and literary figures and their works. The team intended their humour to be impossible to categorise, and succeeded so completely that the adjective \\\"Pythonesque\\\" was invented to defi\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 17:chain:LLMChain > 18:llm:ChatOpenAI] [574ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"The first name of the second customer in the list is \\\"Lang\\\"\\nFinal Answer: Lang\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"The first name of the second customer in the list is \\\"Lang\\\"\\nFinal Answer: Lang\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 18,\n",
            "      \"prompt_tokens\": 3148,\n",
            "      \"total_tokens\": 3166\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 17:chain:LLMChain] [577ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"The first name of the second customer in the list is \\\"Lang\\\"\\nFinal Answer: Lang\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [9.83s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Lang\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 266
        }
      ],
      "source": [
        "# External sorting\n",
        "customer_list.sort(key=lambda x: (x[1], x[0]))\n",
        "\n",
        "# Simple command for the agent\n",
        "sorted_customers = str(customer_list)  # Convert to string if necessary\n",
        "agent.run(f\"Here are the sorted customers: {sorted_customers}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2a4685"
      },
      "source": [
        "## Define your own tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "height": 47,
        "tags": [],
        "id": "73f8b595"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "height": 166,
        "tags": [],
        "id": "61c64268"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\"\n",
        "    return str(date.today())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "height": 115,
        "tags": [],
        "id": "0d2fd9ee"
      },
      "outputs": [],
      "source": [
        "agent= initialize_agent(\n",
        "    tools + [time],\n",
        "    llm,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "height": 81,
        "tags": [],
        "id": "1fcd13cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad045175-9943-4ee5-a172-f56e9ec8a9e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"whats the date today?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"whats the date today?\",\n",
            "  \"agent_scratchpad\": \"\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\ntime: time(text: str) -> str - Returns todays date, use this for any     questions related to knowing todays date.     The input should always be an empty string,     and this function will always return todays     date - any date mathmatics should occur     outside this function.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia, time\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: whats the date today?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain > 3:llm:ChatOpenAI] [687ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Question: whats the date today?\\nThought: I can use the `time` tool to get today's date\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"time\\\",\\n  \\\"action_input\\\": \\\"\\\"\\n}\\n```\\n\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Question: whats the date today?\\nThought: I can use the `time` tool to get today's date\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"time\\\",\\n  \\\"action_input\\\": \\\"\\\"\\n}\\n```\\n\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 44,\n",
            "      \"prompt_tokens\": 377,\n",
            "      \"total_tokens\": 421\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 2:chain:LLMChain] [692ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"Question: whats the date today?\\nThought: I can use the `time` tool to get today's date\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"time\\\",\\n  \\\"action_input\\\": \\\"\\\"\\n}\\n```\\n\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:time] Entering Tool run with input:\n",
            "\u001b[0m\"\"\n",
            "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 4:tool:time] [1ms] Exiting Tool run with output:\n",
            "\u001b[0m\"2024-04-30\"\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"input\": \"whats the date today?\",\n",
            "  \"agent_scratchpad\": \"This was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: whats the date today?\\nThought: I can use the `time` tool to get today's date\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"time\\\",\\n  \\\"action_input\\\": \\\"\\\"\\n}\\n```\\n\\nObservation: 2024-04-30\\nThought:\",\n",
            "  \"stop\": [\n",
            "    \"Observation:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"System: Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\ntime: time(text: str) -> str - Returns todays date, use this for any     questions related to knowing todays date.     The input should always be an empty string,     and this function will always return todays     date - any date mathmatics should occur     outside this function.\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \\\"action\\\" field are: Calculator, wikipedia, time\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{\\n  \\\"action\\\": $TOOL_NAME,\\n  \\\"action_input\\\": $INPUT\\n}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.\\nHuman: whats the date today?\\n\\nThis was your previous work (but I haven't seen any of it! I only see what you return as final answer):\\nQuestion: whats the date today?\\nThought: I can use the `time` tool to get today's date\\nAction:\\n```\\n{\\n  \\\"action\\\": \\\"time\\\",\\n  \\\"action_input\\\": \\\"\\\"\\n}\\n```\\n\\nObservation: 2024-04-30\\nThought:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain > 6:llm:ChatOpenAI] [506ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I have successfully used the `time` tool to get today's date.\\nFinal Answer: Today's date is 2024-04-30.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I have successfully used the `time` tool to get today's date.\\nFinal Answer: Today's date is 2024-04-30.\",\n",
            "            \"additional_kwargs\": {},\n",
            "            \"tool_calls\": [],\n",
            "            \"invalid_tool_calls\": []\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 30,\n",
            "      \"prompt_tokens\": 459,\n",
            "      \"total_tokens\": 489\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo-0301\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor > 5:chain:LLMChain] [509ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"text\": \"I have successfully used the `time` tool to get today's date.\\nFinal Answer: Today's date is 2024-04-30.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:AgentExecutor] [1.21s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Today's date is 2024-04-30.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    result = agent(\"whats the date today?\")\n",
        "except:\n",
        "    print(\"exception on external access\")"
      ]
    }
  ]
}